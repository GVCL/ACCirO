{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.signal import argrelextrema\n",
    "from scipy.ndimage import rank_filter\n",
    "import scipy.stats as st\n",
    "import statsmodels.api as sm\n",
    "import glob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO find the best fit distribution of histogram\n",
    "def best_fit_distribution(data, bins=200, ax=None):\n",
    "    \"\"\"Model data by finding best fit distribution to data\"\"\"\n",
    "    # Get histogram of original data\n",
    "    y, x = np.histogram(data, bins=bins, density=True)\n",
    "    x = (x + np.roll(x, -1))[:-1] / 2.0\n",
    "\n",
    "    # Distributions to check\n",
    "    DISTRIBUTIONS = [        \n",
    "        st.alpha,st.anglit,st.arcsine,st.beta,st.betaprime,st.bradford,st.burr,st.cauchy,st.chi,st.chi2,st.cosine,\n",
    "        st.dgamma,st.dweibull,st.erlang,st.expon,st.exponnorm,st.exponweib,st.exponpow,st.f,st.fatiguelife,st.fisk,\n",
    "        st.foldcauchy,st.foldnorm,st.frechet_r,st.frechet_l,st.genlogistic,st.genpareto,st.gennorm,st.genexpon,\n",
    "        st.genextreme,st.gausshyper,st.gamma,st.gengamma,st.genhalflogistic,st.gilbrat,st.gompertz,st.gumbel_r,\n",
    "        st.gumbel_l,st.halfcauchy,st.halflogistic,st.halfnorm,st.halfgennorm,st.hypsecant,st.invgamma,st.invgauss,\n",
    "        st.invweibull,st.johnsonsb,st.johnsonsu,st.ksone,st.kstwobign,st.laplace,st.levy,st.levy_l,st.levy_stable,\n",
    "        st.logistic,st.loggamma,st.loglaplace,st.lognorm,st.lomax,st.maxwell,st.mielke,st.nakagami,st.ncx2,st.ncf,\n",
    "        st.nct,st.norm,st.pareto,st.pearson3,st.powerlaw,st.powerlognorm,st.powernorm,st.rdist,st.reciprocal,\n",
    "        st.rayleigh,st.rice,st.recipinvgauss,st.semicircular,st.t,st.triang,st.truncexpon,st.truncnorm,st.tukeylambda,\n",
    "        st.uniform,st.vonmises,st.vonmises_line,st.wald,st.weibull_min,st.weibull_max,st.wrapcauchy\n",
    "    ]\n",
    "\n",
    "    # Best holders\n",
    "    best_distribution = st.norm\n",
    "    best_params = (0.0, 1.0)\n",
    "    best_sse = np.inf\n",
    "\n",
    "    # Estimate distribution parameters from data\n",
    "    for distribution in DISTRIBUTIONS:\n",
    "\n",
    "        # Try to fit the distribution\n",
    "        try:\n",
    "            # Ignore warnings from data that can't be fit\n",
    "            with warnings.catch_warnings():\n",
    "                warnings.filterwarnings('ignore')\n",
    "\n",
    "                # fit dist to data\n",
    "                params = distribution.fit(data)\n",
    "\n",
    "                # Separate parts of parameters\n",
    "                arg = params[:-2]\n",
    "                loc = params[-2]\n",
    "                scale = params[-1]\n",
    "\n",
    "                # Calculate fitted PDF and error with fit in distribution\n",
    "                pdf = distribution.pdf(x, loc=loc, scale=scale, *arg)\n",
    "                sse = np.sum(np.power(y - pdf, 2.0))\n",
    "\n",
    "                # if axis pass in add to plot\n",
    "                try:\n",
    "                    if ax:\n",
    "                        pd.Series(pdf, x).plot(ax=ax)\n",
    "                    end\n",
    "                except Exception:\n",
    "                    pass\n",
    "\n",
    "                # identify if this distribution is better\n",
    "                if best_sse > sse > 0:\n",
    "                    best_distribution = distribution\n",
    "                    best_params = params\n",
    "                    best_sse = sse\n",
    "\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    return (best_distribution.name, best_params)\n",
    "\n",
    "\n",
    "# To explain trends if there is any time line in graph\n",
    "def predictTrend(data,ylabels,xlabels,bar_type,x_title,y_title):\n",
    "    Summ=''\n",
    "    for i in range(len(ylabels)):\n",
    "        hgt = data[:,i]\n",
    "        local_max = argrelextrema(hgt, np.greater)\n",
    "        local_min = argrelextrema(hgt, np.less)\n",
    "        order = np.array([0] * len(xlabels))\n",
    "        order[local_min] = -1\n",
    "        order[local_max] = 1\n",
    "            \n",
    "        if ylabels[0]=='Y' and len(ylabels)==1:\n",
    "            trend_str=\". The Y axis value\"\n",
    "            if y_title != '_':\n",
    "                trend_str=\". The \"+y_title\n",
    "        else:\n",
    "            trend_str=\". The \"+ylabels[i]\n",
    "            if y_title != '_':\n",
    "                trend_str=\". The \"+y_title+\" of \"+ylabels[i]\n",
    "            \n",
    "    \n",
    "            \n",
    "        if list(order)==[0]*len(xlabels):\n",
    "            if(int(hgt[0])<int(hgt[1])):\n",
    "                trend_str += \" has an overall increasing trend\"\n",
    "                if( int(hgt[len(hgt)-2])>int(hgt[len(hgt)-1]) ):\n",
    "                    trend_str += \" till \"+str(xlabels[len(xlabels)-2])+\" and ends with a drop in \"+str(xlabels[len(xlabels)-1])\n",
    "                else:\n",
    "                    trend_str += \" from \"+str(xlabels[0])+\" to \"+str(xlabels[len(xlabels)-1])\n",
    "            elif(int(hgt[0])>int(hgt[1])):\n",
    "                trend_str += \" has an overall decreasing trend\"\n",
    "                if( int(hgt[len(hgt)-2])<int(hgt[len(hgt)-1]) ):\n",
    "                    trend_str += \" till \"+str(xlabels[len(xlabels)-2])+\" and ends with a peak in \"+str(xlabels[len(xlabels)-1])\n",
    "                else:\n",
    "                    trend_str += \" from \"+str(xlabels[0])+\" to \"+str(xlabels[len(xlabels)-1])\n",
    "            else:\n",
    "                if(int(hgt[0])!=int(hgt[len(hgt)-1])):\n",
    "                    trend_str += \" is uniform with \"+str(int(hgt[0]))+\" till \"+str(xlabels[len(xlabels)-2])+\" and finally ends with \"+str(int(hgt[len(hgt)-1]))+\" in \"+str(xlabels[len(xlabels)-1])\n",
    "                else:\n",
    "                    trend_str += \" is uniform with \"+str(int(hgt[0]))+\" throughout the entire period\"   \n",
    "        else:\n",
    "            trend_str += \" starts with \"+str(int(hgt[0]))+\" in \"+str(xlabels[0])+\" then \"\n",
    "            j=1\n",
    "            while j<len(order):      \n",
    "                if order[j]==-1:\n",
    "                    if list(order[:j])==[0]*j:\n",
    "                        trend_str += \"declines till \"+str(xlabels[j])+\", followed by \"\n",
    "                    else: \n",
    "                        trend_str += \"a decreasing trend till \"+str(xlabels[j])+\", \"\n",
    "                elif order[j]==1:\n",
    "                    if list(order[:j])==[0]*j:\n",
    "                        trend_str += \"increases till \"+str(xlabels[j])+\", followed by \"\n",
    "                    else : \n",
    "                        trend_str += \"an increasing trend till \"+str(xlabels[j])+\", \"\n",
    "                j+=1\n",
    "            if(order[j-2]!=0):\n",
    "                trend_str += \"and finally ends with \"+str(int(hgt[j-1]))+\" in \"+str(xlabels[j-1])\n",
    "            else :\n",
    "                if(hgt[j-1]<hgt[j-2]):\n",
    "                    trend_str += \"a decreasing trend till \"+str(xlabels[j-1])+\" the end\"\n",
    "                else:\n",
    "                    trend_str += \"an increasing trend till \"+str(xlabels[j-1])+\" the end\"\n",
    "                    \n",
    "                    \n",
    "        # speaking about trend of each group\n",
    "        Summ += trend_str\n",
    "    return Summ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The plot depicts a Horizontal Stacked Bar Graph illustrating Stacked bar chart. The plot is having Total fruit consumption on x-axis for joe, jane, and john. The list of 'Y-axis' values is Bananas, Grapes, Pears, Oranges, and Apples. The 'joe' range from 1.94 to 5.21, with a standard deviation of 1.11. The 'jane' range from 0.79 to 2.98, with a standard deviation of 0.69. The 'john' range from 1.92 to 7.39, with a standard deviation of 1.88. The categories 'joe' and 'john' are negatively correlated by -0.97 Spearman rank correlation\n",
      "The plot depicts a Vertical Stacked Bar Graph illustrating Rating of players aspects. The plot is between Rating out of 100 on y-axis over Players on the x-axis for acceleration, sprintspeed, balance, shotpower, jumping, and stamina. The range of 'Players' values are 1 to 10. The 'acceleration' range from 44.58 to 95.42, with a standard deviation of 15.96. The 'sprintspeed' range from 57.8 to 90.34, with a standard deviation of 11.14. The 'balance' range from 42.54 to 94.41, with a standard deviation of 17.36. The 'shotpower' range from 23.22 to 96.44, with a standard deviation of 23.83. The 'jumping' range from 56.78 to 95.42, with a standard deviation of 12.12. The 'stamina' range from 40.51 to 90.34, with a standard deviation of 18.0. The categories 'acceleration' and 'sprintspeed' are positively correlated by 0.81 Spearman rank correlation. All except for 10 'sprintspeed' is lesser than ''acceleration'. The categories 'acceleration' and 'balance' are positively correlated by 0.76 Spearman rank correlation. The categories 'sprintspeed' and 'shotpower' are positively correlated by 0.7 Spearman rank correlation\n",
      "The plot depicts a Horizontal Stacked Bar Graph illustrating Literacy rate in India. The plot is between State on y-axis over Literacy rate on the x-axis for rural 2001, rural 2011, urban 2001, and urban 2011. The list of 'State' values is Andhra Pradesh, Arunachal Prade, Assam, Bihar, Chhattisgarh, Goa, Gujarat, Haryana, Himachal Prades, Jammu & Kashmir, and Jharkhand. The 'rural 2001' range from 44.19 to 80.09, with a standard deviation of 11.06. The 'rural 2011' range from 59.47 to 86.21, with a standard deviation of 8.76. The 'urban 2001' range from 71.69 to 88.5, with a standard deviation of 5.05. The 'urban 2011' range from 76.27 to 90.79, with a standard deviation of 4.68. The categories 'rural 2001' and 'rural 2011' are positively correlated by 0.91 Spearman rank correlation. The categories 'rural 2001' and 'urban 2001' are positively correlated by 0.74 Spearman rank correlation. The categories 'rural 2001' and 'urban 2011' are positively correlated by 0.82 Spearman rank correlation. The categories 'rural 2011' and 'urban 2001' are positively correlated by 0.81 Spearman rank correlation. All except for Goa 'urban 2001' is greater than 'rural 2011'. The categories 'rural 2011' and 'urban 2011' are positively correlated by 0.85 Spearman rank correlation. The categories 'urban 2001' and 'urban 2011' are positively correlated by 0.98 Spearman rank correlation\n",
      "The plot depicts a Horizontal Stacked Bar Graph illustrating Military expenditure across countries. The plot is between Country on y-axis over Cost in lakhs on the x-axis for 1960, 1961, 1962, 1963, and 1964. The list of 'Country' values is Peru, Thailand, Malaysia, Jordan, Nigeria, Sudan, Ireland, Ghana, Libya, Ecuador, and Sri Lanka. The '1960' range from 51.4 to 633.18, with a standard deviation of 182.29. The '1961' range from 0.0 to 626.17, with a standard deviation of 187.78. The '1962' range from 170.56 to 815.42, with a standard deviation of 202.48. The '1963' range from 149.53 to 976.64, with a standard deviation of 269.15. The '1964' range from 142.52 to 1200.93, with a standard deviation of 317.01. The categories '1960' and '1961' are positively correlated by 0.9 Spearman rank correlation. The categories '1960' and '1962' are positively correlated by 0.89 Spearman rank correlation. The categories '1960' and '1963' are positively correlated by 0.84 Spearman rank correlation. The categories '1960' and '1964' are positively correlated by 0.83 Spearman rank correlation. The categories '1961' and '1962' are positively correlated by 0.96 Spearman rank correlation. The categories '1961' and '1963' are positively correlated by 0.97 Spearman rank correlation. The categories '1961' and '1964' are positively correlated by 0.95 Spearman rank correlation. All except for Sri Lanka '1964' is greater than '1961'. The categories '1962' and '1963' are positively correlated by 0.98 Spearman rank correlation. The categories '1962' and '1964' are positively correlated by 0.95 Spearman rank correlation. The categories '1963' and '1964' are positively correlated by 0.95 Spearman rank correlation\n"
     ]
    }
   ],
   "source": [
    "path = \"/Users/daggubatisirichandana/PycharmProjects/SurveyApplication/CSurveyApp_V2/static/data/SrvyTstData/type4/\"\n",
    "\n",
    "for file in glob.glob(path+\"*.csv\"):\n",
    "    imgno = file.split('/')[-1].split(\".\")[0].split(\"_\")[-1]\n",
    "    df = pd.read_csv(file)\n",
    "#     print(df)\n",
    "    \n",
    "    xlabel = (df.loc[ : , list(df)[0]]).values\n",
    "    xlabels = []\n",
    "    for i in xlabel:\n",
    "        if isinstance(i, np.float64):\n",
    "            xlabels += [int(round(i))]\n",
    "        else :\n",
    "            xlabels += [i]\n",
    "\n",
    "    ylabels = list(df)[1:len(list(df))-4]\n",
    "\n",
    "    data = (df.loc[ : , ylabels]).values\n",
    "    x_title = df['x-title'][0]\n",
    "    y_title = df['y-title'][0]\n",
    "    title = df['title'][0]\n",
    "    bar_type = df['bar_type'][0]\n",
    "\n",
    "\n",
    "    if bar_type == 'Histogram':\n",
    "        min_w = round(min(list(df['bin_width'])),2)\n",
    "        freq = []\n",
    "        for i in np.array(df.ix[:,0:3]):\n",
    "            freq+=[int(i[0])]*int(i[1])\n",
    "        data = pd.Series(freq) \n",
    "        mode_id = list(df['freq']).index(max(list(df['freq']))) \n",
    "        if(title == '_'):\n",
    "            Summ = 'The plot depicts a '+bar_type+' with the bins ranging from '\n",
    "        else:\n",
    "            Summ = 'The plot depicts a '+bar_type+' of '+title+'. The bins range from '\n",
    "        best_fit_name, best_fit_params = best_fit_distribution(data, len(df), None)\n",
    "        best_dist = getattr(st, best_fit_name)\n",
    "        param_names = (best_dist.shapes + ', loc, scale').split(', ') if best_dist.shapes else ['loc', 'scale']\n",
    "        param_str = ', '.join(['{}={:0.2f}'.format(k,v) for k,v in zip(param_names, best_fit_params)])\n",
    "\n",
    "        Summ += str(round(df['bin_center'][0]))+' to '+str(round(df['bin_center'][len(df)-1]))+' with '+str(min_w)+' bin width. The mode of a histogram is '+str(round(df['bin_center'][mode_id]))+' with a frequency of '+str(int(round(df['freq'][mode_id])))+'. The frequency distribution of histogram is the '+best_fit_name+' with following parameters '+param_str+'.'\n",
    "\n",
    "    else:\n",
    "        Summ = 'The plot depicts a '+bar_type+' Graph'\n",
    "\n",
    "        if title !='_':\n",
    "            Summ += ' illustrating '+title\n",
    "\n",
    "        if x_title != '_' and y_title != '_':\n",
    "            Summ +='. The plot is between '+y_title+' on y-axis over '+x_title+' on the x-axis'\n",
    "        elif y_title != '_':\n",
    "            Summ +='. The plot is having '+y_title+' on y-axis'\n",
    "        elif x_title != '_':\n",
    "            Summ +='. The plot is having '+x_title+' on x-axis'\n",
    "\n",
    "        # speaking about legend\n",
    "        if bar_type != 'Vertical Simple Bar' and bar_type != 'Horizontal Simple Bar':\n",
    "            Summ +=' for '\n",
    "            for i in range(len(ylabels)-1):\n",
    "                Summ += str(ylabels[i])+\", \"\n",
    "            Summ += \"and \"+str(ylabels[i+1])\n",
    "\n",
    "        if bar_type == 'Horizontal Simple Bar' or bar_type == 'Horizontal Grouped Bar' or bar_type == 'Horizontal Stacked Bar':\n",
    "            temp = y_title\n",
    "            y_title = x_title\n",
    "            x_title = temp\n",
    "\n",
    "        # To speak about trend in graph if it has ordered attributes, or It is already in sorted order\n",
    "        trend_found = False\n",
    "        # the attributes whose order of values is important\n",
    "        order_attr = ['date','year','month','day','age']\n",
    "        # check if any title has this attribute \n",
    "        for i in order_attr :\n",
    "            if (i in y_title.lower()) or (i in x_title.lower()):\n",
    "                trend_found=True\n",
    "                Summ += predictTrend(data,ylabels,xlabels,bar_type,x_title,y_title)\n",
    "                break\n",
    "\n",
    "        if not trend_found:\n",
    "            if bar_type == 'Horizontal Simple Bar' or bar_type == 'Vertical Simple Bar':\n",
    "                # Graph is already in sorted order\n",
    "                if ((data[:,0]==sorted(data[:,0])).all() or (data[:,0]==sorted(data[:,0],reverse=True)).all()):\n",
    "                    trend_found=True\n",
    "                    Summ += predictTrend(data,ylabels,xlabels,bar_type,x_title,y_title)\n",
    "                else: \n",
    "                    dat, xlabels= zip(*sorted(zip(np.round(data[:,0].tolist(), decimals=2), xlabels), reverse=True))\n",
    "                    if x_title != '_':\n",
    "                        if y_title == '_':\n",
    "                            y_title = 'value'\n",
    "                        Summ += '. The '+x_title+' with the highest '+y_title+' '+str(dat[0])+' is \\''+str(xlabels[0])+'\\'. The '+x_title+' with the lowest '+y_title+' '+str(dat[len(dat)-1])+' is \\''+str(xlabels[len(dat)-1])+'\\'. The mean '+y_title+' of '+x_title+' is '+str(round(sum(dat)/len(dat),2))\n",
    "                    else:\n",
    "                        if y_title == '_':\n",
    "                            y_title = 'value'\n",
    "                        Summ += '. The highest '+y_title+' '+str(dat[0])+' is \\''+str(xlabels[0])+'\\'. The lowest '+y_title+' '+str(dat[len(dat)-1])+' is \\''+str(xlabels[len(dat)-1])+'\\'. The mean '+y_title+' is '+str(round(sum(dat)/len(dat),2))\n",
    "\n",
    "            # For Catogeorical Graphs\n",
    "            else:\n",
    "                # To speak about x axis labels\n",
    "                if x_title == '_':\n",
    "                    if 'Vertical' in bar_type:\n",
    "                        x_title = 'X-axis'\n",
    "                    else :\n",
    "                        x_title = 'Y-axis' \n",
    "                if(isinstance(xlabels[0], str)):\n",
    "                    Summ +='. The list of \\''+x_title+'\\' values is '\n",
    "                    for i in range(len(xlabels)-1):\n",
    "                        Summ += xlabels[i]+', '\n",
    "                    Summ += 'and '+xlabels[-1]\n",
    "                else:\n",
    "                    Summ += '. The range of \\''+x_title+'\\' values are '+str(xlabels[0])+' to '+str(xlabels[len(xlabels)-1])\n",
    "\n",
    "                # To represent ranges of all groups\n",
    "                for i in range(len(ylabels)):         \n",
    "                    Summ += '. The \\''+str(ylabels[i])+'\\' range from '+str(round(min(data[:,i]),2))+' to '+str(round(max(data[:,i]),2))+', with a standard deviation of '+str(round(np.std(data[:,i]),2))            \n",
    "\n",
    "                # Check for Correlation \n",
    "                corr_mat = np.triu(df.iloc[:,1:len(list(df))-4].corr(method='spearman'), k=1)\n",
    "                x,y=np.nonzero(abs(corr_mat)>0.6)\n",
    "                for j in range(len(x)):\n",
    "                    if corr_mat[x[j],y[j]]>0:\n",
    "                        Summ += '. The categories \\''+str(ylabels[x[j]])+\"\\' and \\'\"+str(ylabels[y[j]])+'\\' are positively correlated by '+str(round(corr_mat[x[j],y[j]],2))+' Spearman rank correlation'\n",
    "                    else:\n",
    "                        Summ += '. The categories \\''+str(ylabels[x[j]])+\"\\' and \\'\"+str(ylabels[y[j]])+'\\' are negatively correlated by '+str(round(corr_mat[x[j],y[j]],2))+' Spearman rank correlation'\n",
    "                    pos = np.count_nonzero((data[:,x[j]]-data[:,y[j]])>0)\n",
    "                    neg = np.count_nonzero((data[:,x[j]]-data[:,y[j]])<0)\n",
    "                    if pos<neg and pos == 1:\n",
    "                        k = np.nonzero((data[:,x[j]]-data[:,y[j]])>0)[0][0]\n",
    "                        Summ += '. All except for '+str(xlabels[k])+' \\''+str(ylabels[y[j]])+'\\' is greater than \\''+str(ylabels[x[j]])+'\\''\n",
    "                    elif neg<pos and neg == 1:\n",
    "                        k = np.nonzero((data[:,x[j]]-data[:,y[j]])<0)[0][0]\n",
    "                        Summ += '. All except for '+str(xlabels[k])+' \\''+str(ylabels[y[j]])+'\\' is lesser than \\'\\''+str(ylabels[x[j]])+'\\''\n",
    "                    elif(np.count_nonzero((data[:,x[j]]-data[:,y[j]])<0) == 1):\n",
    "                        k = np.nonzero((data[:,x[j]]-data[:,y[j]])==0)[0][0]\n",
    "                        Summ += '. All except for '+str(xlabels[k])+' \\''+str(ylabels[y[j]])+'\\' is equal to \\''+str(ylabels[x[j]])+'\\''\n",
    "    \n",
    "    print(imgno)\n",
    "    print(Summ)\n",
    "    print(\"________________________________________________________________________\")\n",
    "\n",
    "    text_file = open(path+\"Summary_\"+str(imgno)+\".txt\", \"w\")\n",
    "    n = text_file.write(Summ)\n",
    "    text_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
